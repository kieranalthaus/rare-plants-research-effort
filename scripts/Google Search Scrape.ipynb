{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from urllib.request import urlopen as uReq\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from urllib.request import urlopen as uReq\n",
    "import requests\n",
    "import re\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import bs4\n",
    "\n",
    "libraries = [\n",
    "    pd,\n",
    "    soup,\n",
    "    requests,\n",
    "    re,\n",
    "    uReq,\n",
    "    datetime\n",
    "]\n",
    "\n",
    "def get_version(lib):\n",
    "    if hasattr(lib, '__version__'):\n",
    "        return lib.__version__\n",
    "    elif lib.__name__ == 'BeautifulSoup':\n",
    "        return f\"bs4 {bs4.__version__}\"\n",
    "    elif lib.__name__ in ['re', 'urlopen', 'datetime']:\n",
    "        return f\"Part of Python {sys.version.split()[0]}\"\n",
    "    else:\n",
    "        return \"Version not available\"\n",
    "\n",
    "for lib in libraries:\n",
    "    print(f\"{lib.__name__}: {get_version(lib)}\")\n",
    "\n",
    "print(f\"\\nPython version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Building the URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm using scraperapi here as a kind of VPN. Basically I am passing a url through there service so I can avoid being locked out. The google scholar url works like this:\n",
    "\n",
    "```\n",
    "https://scholar.google.ch/scholar?hl=en&q=[\"SPECIES NAME\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://api.scraperapi.com?api_key=YOUR_API_KEY_HERE&url=https://scholar.google.ch/scholar?hl=en&q=\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Google Scholar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am passing the url I built above and downloading the html. The psoup variable is converting it to a beautiful soup object so we can interact with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "uClient = uReq(url+name)\n",
    "page_html = uClient.read()\n",
    "uClient.close()\n",
    "psoup = soup(page_html, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching the psoup object to find the specifgic div class identified in google scholar on Chrome. It returns several results, so I use the \"[1]\" at the end to pick the right one in the list. Adding the \".text\" removes the html tags and only returns the text. As you can see it spits out the line at the top of the search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsRaw = psoup.find_all('div',class_='gs_ab_mdw')[1].text\n",
    "resultsRaw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using regex to pull out just the number from the regular string. The regex expression breaks up the sentence in to three parts, first \"About \", then the results (which I grab by using a period which represents any character, and an astericks, which allows for any number of characters), then \" results\". Selecting the third object in the list returns the number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = re.search('(About )(.*)( results)',resultsRaw)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Rare Plants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnps = pd.read_csv('CNPSresult20210227.csv',encoding='latin-1')\n",
    "cnps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the moment, I am going to do a really simple analysis and just feed the names in. For future work, however, I will need to consider:\n",
    "1. How does the inclusion of var. and ssp. affect results\n",
    "2. How does variations of subspecies affect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = cnps['Scientific Name']\n",
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a barebones script, pulling the raw text output of the results from Google Scholar. I tried cleaning it within the script but kept running into various edge cases. My current plan now is to pull what I can, record what doesn't work, and do the regex in pandas after the fact. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for spp in names:\n",
    "    name = spp.replace(\" \",\"%20\")\n",
    "    name = '\"'+name+'\"'\n",
    "    \n",
    "    uClient = uReq(url+name)\n",
    "    page_html = uClient.read()\n",
    "    uClient.close()\n",
    "    psoup = soup(page_html, \"lxml\")\n",
    "    \n",
    "    try: \n",
    "        result = psoup.find_all('div',class_='gs_ab_mdw')[1].text\n",
    "    except: \n",
    "        result = 'Failed'\n",
    "        \n",
    "    results.append([spp,result])\n",
    "    print(spp,result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above quit after about 500 names. So I took where it left off and wrote a new script that is a bit more robust. It will attempt to pull just the text, and if it doesnt work for whatever reason, it will move on with a simple \"Failed\" result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names[504:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = []\n",
    "\n",
    "for spp in names[504:]:\n",
    "    name = spp.replace(\" \",\"%20\")\n",
    "    name = '\"'+name+'\"'\n",
    "    try: \n",
    "        uClient = uReq(url+name)\n",
    "        page_html = uClient.read()\n",
    "        uClient.close()\n",
    "        psoup = soup(page_html, \"lxml\")\n",
    "\n",
    "        result = psoup.find_all('div',class_='gs_ab_mdw')[1].text\n",
    "    except: \n",
    "        result = 'Failed'\n",
    "        \n",
    "    results2.append([spp,result])\n",
    "    now = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(spp,result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took awhile, but it got through the list. Below I am combining the results of both scrapes. I first make a dataframe for each results list, then use pd.concat to combine into the new dataframe 'r' (for rare plants)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results,columns=['name','result'])\n",
    "df2 = pd.DataFrame(results2,columns=['name','result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.concat([df,df2])\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I make a quick csv to because I'm nervous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.to_csv('rawResults20210228.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch Regex code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    count = re.search('(About )(.*)( results)',resultsRaw)[2]\n",
    "except:\n",
    "    count = re.search('(.*)( results)',resultsRaw)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating the r dataframe based on the add dataframe. You need to set the index on name first for it to work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r.set_index('name',inplace=True)\n",
    "add.set_index('name',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.update(add,join='left',overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New count of error species after the update. Matches with the add dataframe!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Thread\n",
    "Picking this up on March 7, 2021. Working to make the script more reliable. Will mostly start fresh from this point on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from urllib.request import urlopen as uReq\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnps = pd.read_csv('CNPSresult20210227.csv',encoding='latin-1')\n",
    "cnps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnps['searchName'] = cnps['Scientific Name']\n",
    "cnps['searchName'] = cnps['searchName'].str.replace('var\\.','*')\n",
    "cnps['searchName'] = cnps['searchName'].str.replace('ssp\\.','*')\n",
    "cnps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnps[cnps['Scientific Name'].str.contains('ssp\\.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = cnps['searchName']\n",
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "total = len(names)\n",
    "current = 0\n",
    "\n",
    "url = \"http://api.scraperapi.com?api_key=YOUR_API_KEY_HERE&url=https://scholar.google.ch/scholar?hl=en&q=\"\n",
    "\n",
    "print('Starting script: '+ datetime.now().strftime(\"%H:%M:%S\"))\n",
    "\n",
    "for spp in names:\n",
    "    name = spp.replace(\" \",\"%20\")\n",
    "    name = '\"'+name+'\"'\n",
    "    \n",
    "    check = False\n",
    "    for attempt in range(10):\n",
    "        while check is False:\n",
    "            try: \n",
    "                uClient = uReq(url+name)\n",
    "                page_html = uClient.read()\n",
    "                uClient.close()\n",
    "                psoup = soup(page_html, \"lxml\")\n",
    "\n",
    "                result = psoup.find_all('div',class_='gs_ab_mdw')[1].text\n",
    "                check = len(result) > 1\n",
    "            except Exception: \n",
    "                continue\n",
    "            break\n",
    "        \n",
    "    results.append([spp,result])\n",
    "    now = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    current += 1\n",
    "    \n",
    "    print(current,'/',total,now,spp,result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results, columns=['name','result'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I checked each one of these results manually, and none had results. Will convert to zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[~df['result'].str.contains('result')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['result'] = df['result'].replace('','0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[~df['result'].str.contains('result')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('scholarResults20210307.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('scholarResults20210307.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['count'] = df['result'].str.extract('(.*) result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['count'] = df['count'].str.replace(',','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['count'] = df['count'].str.replace('About ','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[~df['count'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['count'] = df['count'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[1849,['count']] = 124000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[1847:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['count'] = pd.to_numeric(df['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cnps.merge(df,how='left',left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Scientific Name','Common Name','CRPR','count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CRPRgen'] = df['CRPR'].str.extract('(..)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x='CRPRgen',y='count',data=df[df['count']<1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['count'][df['count'] < 100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "df['count'][df['count'] < 100000].hist(ax=ax, bins=50, bottom=0.1)\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('RareSppCounts20210307.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['count']>5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
